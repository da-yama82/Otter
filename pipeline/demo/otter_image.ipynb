{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otter Image Demo (In-context Learning)\n",
    "\n",
    "Here is an example of multi-modal ICL (in-context learning) with ü¶¶ Otter. We provide two demo images with corresponding instructions and answers, then we ask the model to generate an answer given our instruct. You may change your instruction and see how the model responds.\n",
    "\n",
    "You can also try our [online demo](https://otter.cliangyu.com/) to see more in-context learning demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂøÖË¶Å„Å™„É¢„Ç∏„É•„Éº„É´„ÅØÂêÑËá™„Ç§„É≥„Çπ„Éà„Éº„É´<br>\n",
    "mlflow==2.6.0„ÅØ„Éê„Ç∞„Åå„ÅÇ„Çã„Åü„ÇÅ‰Ωø„Çè„Å™„ÅÑ„Åì„Å®(https://github.com/mlflow/mlflow/issues/9331) (2023.08.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade mlflow==2.5.0 pydantic==1.10.12 deepspeed==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ueno/.conda/envs/otter/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-28 04:51:02,409] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from otter.modeling_otter import OtterForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ë™≠„ÅøËæº„Åø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model version is configured for Otter-Image with max_num_frames set to None.\n",
      "Total Trainable param: 1.385404 B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:18<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = OtterForConditionalGeneration.from_pretrained(\"luodian/OTTER-Image-MPT7B\", device_map=\"auto\") # Hugging Face\n",
    "model = OtterForConditionalGeneration.from_pretrained(\"/data/dataset/otter/OTTER-Image-MPT7B/\", device_map=\"auto\")\n",
    "tokenizer = model.text_tokenizer\n",
    "image_processor = transformers.CLIPImageProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### „Éà„Éº„ÇØ„É≥„ÅÆÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', '<PAD>', '<|endofchunk|>', '<image>', '<answer>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of GPTNeoXTokenizerFast(name_or_path='mosaicml/mpt-7b-instruct', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<|endofchunk|>', '<image>', '<answer>']}, clean_up_tokenization_spaces=True)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_tokenizer.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.convert_ids_to_tokens of GPTNeoXTokenizerFast(name_or_path='mosaicml/mpt-7b-instruct', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<|endofchunk|>', '<image>', '<answer>']}, clean_up_tokenization_spaces=True)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_bos_token', '_build_conversation_input_ids', '_call_one', '_cls_token', '_convert_encoding', '_convert_id_to_token', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenizer', '_unk_token', '_upload_modified_files', 'add_prefix_space', 'add_special_tokens', 'add_tokens', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'as_target_tokenizer', 'backend_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'can_save_slow_tokenizer', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'set_truncation_and_padding', 'slow_tokenizer_class', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'train_new_from_iterator', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'verbose', 'vocab', 'vocab_files_names', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "attributes_and_methods = dir(model.text_tokenizer)\n",
    "print(attributes_and_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Â≠¶ÁøíÊ∏à„ÅøÈáç„Åø„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ckpt_path = '../../log/VI_batch128_long_pairs25/final_weights.pt'\n",
    "# trained_ckpt_path = '../../weights/OTTER-Image-MPT7B/final_weights.pt' # „Éá„Éï„Ç©„É´„Éà\n",
    "\n",
    "train_ckpt = torch.load(trained_ckpt_path, map_location=\"cpu\")\n",
    "if train_ckpt.get(\"model_state_dict\", None) is not None:\n",
    "    train_ckpt = train_ckpt[\"model_state_dict\"]\n",
    "_ = model.load_state_dict(train_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ckpt.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÁîªÂÉè„Å®„Éó„É≠„É≥„Éó„Éà„ÅÆÁî®ÊÑè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "demo_image_two = Image.open(requests.get(\"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\", stream=True).raw)\n",
    "query_image = Image.open(requests.get(\"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", stream=True).raw)\n",
    "\n",
    "vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "model.text_tokenizer.padding_side = \"left\"\n",
    "lang_x = model.text_tokenizer(\n",
    "    [\n",
    "        \"<image>User: a photo of GPT:<answer> two cats sleeping.<|endofchunk|><image>User: a photo of GPT:<answer> a bathroom sink.<|endofchunk|><image>User: a photo of GPT:<answer>\"\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(vision_x.shape) # torch.Size([1, 3, 1, 3, 224, 224]) shape (B, num_imgs, Frames=1, C, H, W)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].imshow(demo_image_one)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(demo_image_two)\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(query_image)\n",
    "axes[2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get the data type from model's parameters\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "# Convert tensors to the model's data type\n",
    "vision_x = vision_x.to(dtype=model_dtype)\n",
    "lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x.to(model.device),\n",
    "    lang_x=lang_x_input_ids.to(model.device),\n",
    "    attention_mask=lang_x_attention_mask.to(model.device),\n",
    "    max_new_tokens=512,\n",
    "    num_beams=3,\n",
    "    no_repeat_ngram_size=3,\n",
    "    bad_words_ids=bad_words_id,\n",
    ")\n",
    "\n",
    "parsed_output = (\n",
    "    model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Âì∫‰π≥È°û„ÅãÂì∫‰π≥È°û„Åß„Å™„ÅÑ„Åã„ÅÆÊé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_image_paths(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ÁîªÂÉè„ÅÆÊã°ÂºµÂ≠ê„É™„Çπ„Éà\n",
    "    all_files = sorted(os.listdir(folder_path)) # „Éï„Ç©„É´„ÉÄÂÜÖ„ÅÆÂÖ®„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó\n",
    "    image_paths = [os.path.join(folder_path, file) for file in all_files if os.path.splitext(file)[1].lower() in image_extensions] # ÁîªÂÉè„ÅÆ„Éë„Çπ„ÇíÊäΩÂá∫„Åó„Å¶„É™„Çπ„Éà„Å´Ê†ºÁ¥ç\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(\"../../../data/test_kosmos/animal_or_not/dog.jpg\")\n",
    "demo_image_two = Image.open(\"../../../data/test_kosmos/animal_or_not/home.jpg\")\n",
    "query_folder_path = \"../../../data/test_kosmos/animal_or_not/\"\n",
    "query_image_paths = get_image_paths(query_folder_path)\n",
    "\n",
    "for i, query_image_path in enumerate(query_image_paths[:]):\n",
    "    query_image = Image.open(query_image_path)\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: Is the first image a mammal? Please answer with Yes or No. GPT:<answer> Yes.<|endofchunk|>\n",
    "        <image>User: Is the second image a mammal? Please answer with Yes or No. GPT:<answer> No.<|endofchunk|>\n",
    "        <image>User: Is the next image a mammal? Please answer with Yes or No. GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Âì∫‰π≥È°û„Åß„ÅÇ„Çå„Å∞A, Âì∫‰π≥È°û„ÅßÁÑ°„Åë„Çå„Å∞B„ÅÆÊé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(\"../../../data/test_kosmos/animal_or_not/dog.jpg\")\n",
    "demo_image_two = Image.open(\"../../../data/test_kosmos/animal_or_not/home.jpg\")\n",
    "query_folder_path = \"../../../data/test_kosmos/animal_or_not/\"\n",
    "query_image_paths = get_image_paths(query_folder_path)\n",
    "\n",
    "for i, query_image_path in enumerate(query_image_paths[:]):\n",
    "    query_image = Image.open(query_image_path)\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: If the subject in this image is a mammal, answer A. If not, answer B. GPT:<answer> A. This is a dog. Dogs are mammals.<|endofchunk|>\n",
    "        <image>User: If the subject in this image is a mammal, answer A. If not, answer B. GPT:<answer> B. This is a house. Houses are not mammals.<|endofchunk|>\n",
    "        <image>User: If the subject in this image is a mammal, answer A. If not, answer B. GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json„Éï„Ç°„Ç§„É´„ÅÆÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã_instructions.json\n",
    "\n",
    "import orjson\n",
    "\n",
    "mimicit_path=\"../../data/LA/LACR_I2I_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    dataset = orjson.loads(f.read())\n",
    "    # dataset = orjson.loads(f.read())[\"data\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã.json\n",
    "\n",
    "import ijson\n",
    "\n",
    "images = {}\n",
    "images_path=\"../../data/LA/LA.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Åã„ÇâÁîªÂÉèÂèØË¶ñÂåñ\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# base64„Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Éá„Éº„Çø\n",
    "str_data1 = images[\"LA_IMG_000000215677\"]\n",
    "str_data2 = images[\"LA_IMG_000000429446\"]\n",
    "\n",
    "# „Éê„Ç§„Éà„Éá„Éº„Çø„Å´„Éá„Ç≥„Éº„Éâ\n",
    "decoded_data1 = base64.b64decode(str_data1)\n",
    "decoded_data2 = base64.b64decode(str_data2)\n",
    "\n",
    "# „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "image1 = Image.open(BytesIO(decoded_data1))\n",
    "image2 = Image.open(BytesIO(decoded_data2))\n",
    "\n",
    "# 2x1„ÅÆsubplot„Çí‰ΩúÊàê„Åó„Å¶„ÄÅ2Êûö„ÅÆÁîªÂÉè„ÇíË°®Á§∫\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "\n",
    "axarr[0].imshow(image1)\n",
    "axarr[0].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "axarr[1].imshow(image2)\n",
    "axarr[1].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã_train.json\n",
    "\n",
    "import orjson\n",
    "\n",
    "train_config_path=\"../../data/LA/LACR_I2I_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "cache_train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_list = list(cache_train_config.keys())\n",
    "print(len(cache_train_list))\n",
    "print(cache_train_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['LACR_I2I_INS_000000296754']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['LACR_I2I_INS_000000222475']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëá™‰Ωú„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "\n",
    "mimicit_path=\"/home/data/MIMIC-IT/VI/train_VI_long_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    dataset = orjson.loads(f.read())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/home/data/MIMIC-IT/VI/train_VI.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# base64„Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Éá„Éº„Çø\n",
    "str_data1 = images[\"metal+metal+image_55\"]\n",
    "str_data2 = images[\"metal+metal_rust+image_8\"]\n",
    "\n",
    "# „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "image1 = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "image2 = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "\n",
    "# 2x1„ÅÆsubplot„Çí‰ΩúÊàê„Åó„Å¶„ÄÅ2Êûö„ÅÆÁîªÂÉè„ÇíË°®Á§∫\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "\n",
    "axarr[0].imshow(image1)\n",
    "axarr[0].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "axarr[1].imshow(image2)\n",
    "axarr[1].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "\n",
    "train_config_path=\"/home/data/MIMIC-IT/VI/train_VI_pairs25_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "cache_train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_list = list(cache_train_config.keys())\n",
    "print(len(cache_train_list))\n",
    "print(cache_train_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['book+aged_book+image_2=0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëá™‰Ωú„Éá„Éº„Çø„Çª„ÉÉ„ÉàÈáç„ÅøÊÄßËÉΩË™øÊüª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ckpt_path = '../../log/VI_batch128_long_pairs25/final_weights.pt'\n",
    "# trained_ckpt_path = '../../weights/OTTER-Image-MPT7B/final_weights.pt' # Â≠¶ÁøíÂâç\n",
    "\n",
    "train_ckpt = torch.load(trained_ckpt_path, map_location=\"cpu\")\n",
    "if train_ckpt.get(\"model_state_dict\", None) is not None:\n",
    "    train_ckpt = train_ckpt[\"model_state_dict\"]\n",
    "_ = model.load_state_dict(train_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVTecAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_image_paths(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ÁîªÂÉè„ÅÆÊã°ÂºµÂ≠ê„É™„Çπ„Éà\n",
    "    all_files = sorted(os.listdir(folder_path)) # „Éï„Ç©„É´„ÉÄÂÜÖ„ÅÆÂÖ®„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó\n",
    "    image_paths = [os.path.join(folder_path, file) for file in all_files if os.path.splitext(file)[1].lower() in image_extensions] # ÁîªÂÉè„ÅÆ„Éë„Çπ„ÇíÊäΩÂá∫„Åó„Å¶„É™„Çπ„Éà„Å´Ê†ºÁ¥ç\n",
    "    return image_paths\n",
    "\n",
    "def write_text_file(file_path, text):\n",
    "    with open(file_path, mode=\"a\") as f:\n",
    "        f.write(text+\"\\n\")\n",
    "        \n",
    "def generate_list_string(items):\n",
    "    # „Ç¢„É≥„ÉÄ„Éº„Çπ„Ç≥„Ç¢„Çí„Çπ„Éö„Éº„Çπ„Å´Â§âÊèõ\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long Áî®\n",
    "def test(category, anormaly_reason, anormaly_type, model_name, order):\n",
    "    if category==\"grid\":\n",
    "        category__ = \"metal grid\"\n",
    "    else:\n",
    "        category__ = category\n",
    "    category__ = category__.replace('_', ' ')\n",
    "    for ano_type,ano_reason in zip(anormaly_type,anormaly_reason):\n",
    "        folder_name = f'./result/{category}/{ano_type}/{model_name}'\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        with open(f'{folder_name}/detective.txt', mode='w') as f:\n",
    "            f.close()\n",
    "        with open(f'{folder_name}/non-detective.txt', mode='w') as f:\n",
    "            f.close()\n",
    "        \n",
    "        subfolder_string = generate_list_string(anormaly_reason)\n",
    "        model.text_tokenizer.padding_side = \"left\"\n",
    "        \n",
    "        \"\"\" „ÇØ„Ç®„É™Ôºö‰∏çËâØÂìÅ \"\"\"\n",
    "        if order: # demo_image_one: ËâØÂìÅ, demo_image_two: ‰∏çËâØÂìÅ\n",
    "            sentence = f\"context1: OK, context2: NG, query: NG\"\n",
    "            # print(sentence)\n",
    "            write_text_file(f'{folder_name}/detective.txt',sentence)\n",
    "            demo_image_one = Image.open(f\"/home/data/mvtec/{category}/test/good/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            demo_image_two = Image.open(f\"/home/data/mvtec/{category}/test/{ano_type}/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            # long\n",
    "            inputs = textwrap.dedent(f\"\"\"\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer> No. This {category__} does not have any defects such as {subfolder_string}, so it is non-defective.<|endofchunk|>\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer> Yes. This {category__} has some {ano_reason}, so it is defective.<|endofchunk|>\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer>\n",
    "            \"\"\")\n",
    "            #short\n",
    "            # inputs = textwrap.dedent(f\"\"\"\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer> No. This {category__} does not have any defects such as {subfolder_string}, so it is non-defective.<|endofchunk|>\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer> Yes. This {category__} has some {ano_reason}, so it is defective.<|endofchunk|>\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer>\n",
    "            # \"\"\")\n",
    "        \n",
    "        else: # demo_image_one: ‰∏çËâØÂìÅ, demo_image_two: ËâØÂìÅ\n",
    "            sentence = f\"context1: NG, context2: OK, query: NG\"\n",
    "            # print(sentence)\n",
    "            write_text_file(f'{folder_name}/detective.txt',sentence)\n",
    "            demo_image_one = Image.open(f\"/home/data/mvtec/{category}/test/{ano_type}/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            demo_image_two = Image.open(f\"/home/data/mvtec/{category}/test/good/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            # long\n",
    "            inputs = textwrap.dedent(f\"\"\"\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer> No. This {category__} does not have any defects such as {subfolder_string}, so it is non-defective.<|endofchunk|>\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer> Yes. This {category__} has some {ano_reason}, so it is defective.<|endofchunk|>\n",
    "                <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer>\n",
    "            \"\"\")\n",
    "            # short\n",
    "            # inputs = textwrap.dedent(f\"\"\"\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer> No. This {category__} does not have any defects such as {subfolder_string}, so it is non-defective.<|endofchunk|>\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer> Yes. This {category__} has some {ano_reason}, so it is defective.<|endofchunk|>\n",
    "            #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer>\n",
    "            # \"\"\")\n",
    "        \n",
    "        inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "        lang_x = model.text_tokenizer(\n",
    "            [\n",
    "                inputs\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        write_text_file(f'{folder_name}/detective.txt',f'-----{ano_type} start-----')\n",
    "        write_text_file(f'{folder_name}/detective.txt',\"\")\n",
    "            \n",
    "        query_folder_path = f\"/home/data/mvtec/{category}/test/{ano_type}\"\n",
    "        query_image_paths = get_image_paths(query_folder_path)\n",
    "        count = 0\n",
    "        for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "            # print(query_image_path)\n",
    "            query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "            vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "            # Get the data type from model's parameters\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "            # Convert tensors to the model's data type\n",
    "            vision_x = vision_x.to(dtype=model_dtype)\n",
    "            lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "            lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "            bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x.to(model.device),\n",
    "                lang_x=lang_x_input_ids.to(model.device),\n",
    "                attention_mask=lang_x_attention_mask.to(model.device),\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                no_repeat_ngram_size=3,\n",
    "                bad_words_ids=bad_words_id,\n",
    "            )\n",
    "\n",
    "            parsed_output = (\n",
    "                model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "            )\n",
    "            \n",
    "            if parsed_output.split(\".\")[0].lower()==\"yes\":\n",
    "                count += 1\n",
    "            \n",
    "            write_text_file(f'{folder_name}/detective.txt',query_image_path)\n",
    "            write_text_file(f'{folder_name}/detective.txt',parsed_output)\n",
    "            write_text_file(f'{folder_name}/detective.txt',\"\")\n",
    "            \n",
    "            # print(inputs)\n",
    "            # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "            # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "            # axes[0].imshow(demo_image_one)\n",
    "            # axes[0].axis('off')\n",
    "            # axes[1].imshow(demo_image_two)\n",
    "            # axes[1].axis('off')\n",
    "            # axes[2].imshow(query_image)\n",
    "            # axes[2].axis('off')\n",
    "            # plt.show()\n",
    "            \n",
    "        acc = f\"correct: {count}, total: {len(query_image_paths)-1}, acc: {(count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        print(acc)\n",
    "        \n",
    "        write_text_file(f'{folder_name}/detective.txt',f'-----{ano_type} end-----')\n",
    "        write_text_file(f'{folder_name}/detective.txt',acc)\n",
    "        \n",
    "        \n",
    "        \"\"\" „ÇØ„Ç®„É™ÔºöËâØÂìÅ \"\"\"\n",
    "        if order: # demo_image_one: ËâØÂìÅ, demo_image_two: ‰∏çËâØÂìÅ\n",
    "            sentence = f\"context1: OK, context2: NG, query: OK\"\n",
    "            # print(sentence)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',sentence)\n",
    "        \n",
    "        else: # demo_image_one: ‰∏çËâØÂìÅ, demo_image_two: ËâØÂìÅ\n",
    "            sentence = f\"context1: NG, context2: OK, query: OK\"\n",
    "            # print(sentence)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',sentence)\n",
    "        \n",
    "        write_text_file(f'{folder_name}/non-detective.txt',f'-----{ano_type} start-----')\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',\"\")\n",
    "            \n",
    "        query_folder_path = f\"/home/data/mvtec/{category}/test/good\"\n",
    "        query_image_paths = get_image_paths(query_folder_path)\n",
    "        count = 0\n",
    "        for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "            # print(query_image_path)\n",
    "            query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "            vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "            # Get the data type from model's parameters\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "            # Convert tensors to the model's data type\n",
    "            vision_x = vision_x.to(dtype=model_dtype)\n",
    "            lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "            lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "            bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x.to(model.device),\n",
    "                lang_x=lang_x_input_ids.to(model.device),\n",
    "                attention_mask=lang_x_attention_mask.to(model.device),\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                no_repeat_ngram_size=3,\n",
    "                bad_words_ids=bad_words_id,\n",
    "            )\n",
    "\n",
    "            parsed_output = (\n",
    "                model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "            )\n",
    "            \n",
    "            if parsed_output.split(\".\")[0].lower()==\"no\":\n",
    "                count += 1\n",
    "            \n",
    "            write_text_file(f'{folder_name}/non-detective.txt',query_image_path)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',parsed_output)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',\"\")\n",
    "            \n",
    "            # print(inputs)\n",
    "            # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "            # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "            # axes[0].imshow(demo_image_one)\n",
    "            # axes[0].axis('off')\n",
    "            # axes[1].imshow(demo_image_two)\n",
    "            # axes[1].axis('off')\n",
    "            # axes[2].imshow(query_image)\n",
    "            # axes[2].axis('off')\n",
    "            # plt.show()\n",
    "            \n",
    "        acc = f\"correct: {count}, total: {len(query_image_paths)-1}, acc: {(count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        print(acc)\n",
    "        \n",
    "        write_text_file(f'{folder_name}/non-detective.txt',f'-----{ano_type} end-----')\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Å´Âê´„Åæ„Çå„Çã„Ç´„ÉÜ„Ç¥„É™\n",
    "1. bottle\n",
    "2. carpet\n",
    "3. leather\n",
    "4. tile\n",
    "5. wood\n",
    "\n",
    "„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Å´Âê´„Åæ„Çå„Å™„ÅÑ„Ç´„ÉÜ„Ç¥„É™\n",
    "1. cable\n",
    "2. capsule\n",
    "3. hazelnut\n",
    "4. pill\n",
    "5. screw\n",
    "6. toothbrush\n",
    "7. transistor\n",
    "8. zipper\n",
    "\n",
    "„Ç∞„É¨„Éº\n",
    "1. grid („Éó„É≠„É≥„Éó„Éà„ÅØmetal grid„Å´„Åô„Çã)\n",
    "2. metal_nut (metal„Åå„ÅÇ„Çã nut„ÅØ„ÉÜ„Çπ„Éà„Å´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"wood\"\n",
    "anormaly_reason = [\"scratched wood\",\"stained wood\",\"wood with holes\"]\n",
    "# anormaly_type = [\"scratch\",\"color\",\"hole\"]\n",
    "anormaly_type = [\"scratch\",\"liquid\",\"hole\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"hazelnut\"\n",
    "anormaly_reason = ['cracked hazelnut','scratched hazelnut','hazelnut with holes','hazelnut with white marks']\n",
    "anormaly_type = [\"crack\",\"cut\",\"hole\",\"print\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"bottle\"\n",
    "anormaly_reason = ['broken bottle','contaminated bottle']\n",
    "anormaly_type = [\"broken_large\",\"contamination\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"broken_small\",\"contamination\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"cable\"\n",
    "anormaly_reason = ['bent cable','swapped cable','broken cable','missing cable','poked cable']\n",
    "anormaly_type = [\"bent_wire\",\"cable_swap\",\"cut_inner_insulation\",\"missing_cable\",\"poke_insulation\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"bent_wire\",\"cable_swap\",\"cut_outer_insulation\",\"missing_wire\",\"poke_insulation\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"capsule\"\n",
    "anormaly_reason = ['cracked capsule','misprinted capsule','poked capsule','scratched capsule','damaged capsule']\n",
    "anormaly_type = [\"crack\",\"faulty_imprint\",\"poke\",\"scratch\",\"squeeze\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"carpet\"\n",
    "anormaly_reason = ['stained carpet','cut carpet','carpet with holes','contaminated carpet']\n",
    "anormaly_type = [\"color\",\"cut\",\"hole\",\"metal_contamination\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"color\",\"cut\",\"hole\",\"thread\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"grid\"\n",
    "anormaly_reason = ['bent metal grid','broken metal grid','contaminated metal grid']\n",
    "anormaly_type = [\"bent\",\"broken\",\"glue\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"bent\",\"broken\",\"metal_contamination\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"bent\",\"broken\",\"thread\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"leather\"\n",
    "anormaly_reason = ['stained leather','cut leather','folded leather','poked leather']\n",
    "anormaly_type = [\"color\",\"cut\",\"fold\",\"poke\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"glue\",\"cut\",\"fold\",\"poke\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"metal_nut\"\n",
    "anormaly_reason = ['bent metal nut','stained metal nut','flipped metal nut','scratched metal nut']\n",
    "anormaly_type = [\"bent\",\"color\",\"flip\",\"scratch\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"pill\"\n",
    "anormaly_reason = ['stained pill','contaminated pill','cracked pill','misprinted pill','scratched pill']\n",
    "anormaly_type = [\"color\",\"contamination\",\"crack\",\"faulty_imprint\",\"scratch\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"pill_type\",\"contamination\",\"crack\",\"faulty_imprint\",\"scratch\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"screw\"\n",
    "anormaly_reason = ['stripped screw','scratched screw','broken screw']\n",
    "anormaly_type = [\"manipulated_front\",\"scratch_head\",\"thread_side\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"manipulated_front\",\"scratch_neck\",\"thread_top\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"tile\"\n",
    "anormaly_reason = ['cracked tile','contaminated tile','stained tile']\n",
    "anormaly_type = [\"crack\",\"glue_strip\",\"gray_stroke\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"crack\",\"glue_strip\",\"oil\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"crack\",\"glue_strip\",\"rough\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"toothbrush\"\n",
    "anormaly_reason = ['damaged toothbrush']\n",
    "anormaly_type = [\"defective\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"transistor\"\n",
    "anormaly_reason = ['bent transistor','cut transistor','damaged transistor','misplaced transistor']\n",
    "anormaly_type = [\"bent_lead\",\"cut_lead\",\"damaged_case\",\"misplaced\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"zipper\"\n",
    "anormaly_reason = ['broken zipper','torn zipper','damaged zipper']\n",
    "anormaly_type = [\"broken_teeth\",\"fabric_border\",\"rough\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)\n",
    "\n",
    "anormaly_type = [\"split_teeth\",\"fabric_interior\",\"squeezed_teeth\"]\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÈùûÂÖ¨Èñã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"rice\"\n",
    "anormaly_reason = [\"brokened rice\",\"rice with milky white\"]\n",
    "anormaly_type = [\"broken\",\"milky_white\"]\n",
    "model_name = \"VI_batch128_long_pairs25\"\n",
    "\n",
    "test(category, anormaly_reason, anormaly_type, model_name, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â≠¶Áøí„Å´‰ΩøÁî®„Åó„Å¶„ÅÑ„Å™„ÅÑ„Éá„Éº„Çø„ÅßÊ§úË®º („Ç´„ÉÜ„Ç¥„É™„ÄÅÊ¨†Èô•Âêç„ÅØÊó¢Áü•)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/home/data/MIMIC-IT/VI/val_VI.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/home/data/MIMIC-IT/VI/val_VI_pairs25_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "# mimicit_path=\"/home/data/MIMIC-IT/VI/val_VI_short_instructions.json\"\n",
    "mimicit_path=\"/home/data/MIMIC-IT/VI/val_VI_long_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][context1][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][context2][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\".\")[0].lower()==parsed_output.split(\".\")[0].lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â≠¶Áøí„Å´‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/home/data/MIMIC-IT/VI/train_VI.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/home/data/MIMIC-IT/VI/train_VI_pairs25_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "# mimicit_path=\"/home/data/MIMIC-IT/VI/train_VI_short_instructions.json\"\n",
    "mimicit_path=\"/home/data/MIMIC-IT/VI/train_VI_long_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][context1][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][context2][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\".\")[0].lower()==parsed_output.split(\".\")[0].lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â≠¶Áøí„Å´‰ΩøÁî®„Åó„Å¶„ÅÑ„Å™„ÅÑ„Éá„Éº„Çø„ÅßÊ§úË®º („Ç´„ÉÜ„Ç¥„É™„ÅØÊú™Áü•„ÄÅÊ¨†Èô•Âêç„ÅØ„Åª„ÅºÊó¢Áü•)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/home/data/MIMIC-IT/VI/test_VI.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/home/data/MIMIC-IT/VI/test_VI_pairs5_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "# mimicit_path=\"/home/data/MIMIC-IT/VI/test_VI_short_instructions.json\"\n",
    "mimicit_path=\"/home/data/MIMIC-IT/VI/test_VI_long_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][context1][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][context2][\"instruction\"]} GPT:<answer> {instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\".\")[0].lower()==parsed_output.split(\".\")[0].lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
